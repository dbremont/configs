{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install pciutils"
      ],
      "metadata": {
        "id": "T3Qct1ljsGOD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a1e06fb2-2317-4406-c074-66e849ba335d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 pci.ids pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 343 kB of archives.\n",
            "After this operation, 1,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Fetched 343 kB in 1s (573 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Tl-H_QcLsa_V",
        "outputId": "e672173c-f268-42a1-ba89-b425cd849169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb  5 20:17:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGU1MHWltyD2",
        "outputId": "c2e46294-92d2-4473-de28-0e5d62679513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Download and run the Ollama Linux install script\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ollama serve&\n",
        "#import subprocess\n",
        "#process = subprocess.Popen([\"ollama\", \"serve\"])"
      ],
      "metadata": {
        "id": "3QMgG1RvsrsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yzLuoBktyD4"
      },
      "outputs": [],
      "source": [
        "# Get Ngrok authentication token from colab secrets environment\n",
        "from google.colab import userdata\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "NGROK_DOMAIN = userdata.get('NGROK_DOMAIN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSyOCalCtyD4",
        "outputId": "9f99bea8-23de-4c89-a6f3-35600194f412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.11)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            ">>> starting ngrok config add-authtoken 2qNVgiQyZnX7b2dUJKyfpCoqdqo_3cmJLCAeyN1mGCUcdEJKY\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Install:\n",
        "#  1. aiohttp for concurrent subprocess execution in Jupyter Notebooks\n",
        "#  2. pyngrok for Ngrok wrapper\n",
        "!pip install aiohttp pyngrok\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "# Define run - a helper function to run subcommands asynchronously.\n",
        "# The function takes in 2 arguments:\n",
        "#  1. command\n",
        "#  2. environment variable\n",
        "async def run(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE\n",
        "  )\n",
        "\n",
        "\n",
        "# This function is designed to handle large amounts of text data efficiently.\n",
        "# It asynchronously iterate over lines and print them, stripping and decoding as needed.\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "\n",
        "# Gather the standard output (stdout) and standard error output (stderr) streams of a subprocess and pipe them through\n",
        "# the `pipe()` function to print each line after stripping whitespace and decoding UTF-8.\n",
        "# This allows us to capture and process both the standard output and error messages from the subprocess concurrently.\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "\n",
        "# Authenticate with Ngrok\n",
        "await asyncio.gather(\n",
        "  run(['ngrok', 'config', 'add-authtoken', NGROK_AUTH_TOKEN])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdadMdLntyD5",
        "outputId": "45fe45d8-1b7d-4c4b-a750-b1e849be3b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434 --host-header localhost:11434 --domain summary-seriously-monarch.ngrok-free.app\n",
            "t=2025-02-05T20:19:33+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2025-02-05T20:19:33+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2025-02-05T20:19:33+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is:\n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIM1mbz3oQMKMSO4oX3DjrDqmZlcF2Fp7FTJ5zKtRFlO0\n",
            "\n",
            "2025/02/05 20:19:33 routes.go:1187: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-02-05T20:19:33.999Z level=INFO source=images.go:432 msg=\"total blobs: 0\"\n",
            "time=2025-02-05T20:19:33.999Z level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\n",
            "[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n",
            "\n",
            "[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n",
            "- using env:\texport GIN_MODE=release\n",
            "- using code:\tgin.SetMode(gin.ReleaseMode)\n",
            "\n",
            "[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n",
            "[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n",
            "[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n",
            "[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n",
            "[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n",
            "[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n",
            "[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n",
            "[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\n",
            "[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n",
            "[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n",
            "[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n",
            "[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n",
            "[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n",
            "[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n",
            "time=2025-02-05T20:19:34.001Z level=INFO source=routes.go:1238 msg=\"Listening on 127.0.0.1:11434 (version 0.5.7)\"\n",
            "t=2025-02-05T20:19:34+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "time=2025-02-05T20:19:34.003Z level=INFO source=routes.go:1267 msg=\"Dynamic LLM libraries\" runners=\"[cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx]\"\n",
            "time=2025-02-05T20:19:34.011Z level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\n",
            "t=2025-02-05T20:19:34+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-02-05T20:19:34+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "time=2025-02-05T20:19:34.268Z level=INFO source=types.go:131 msg=\"inference compute\" id=GPU-83d1565a-856c-4425-03cf-49ec8110cc5a library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "t=2025-02-05T20:19:34+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://summary-seriously-monarch.ngrok-free.app\n",
            "t=2025-02-05T20:19:50+0000 lvl=info msg=\"join connections\" obj=join id=474fefd068c2 l=127.0.0.1:11434 r=190.122.106.218:32394\n",
            "[GIN] 2025/02/05 - 20:19:50 | 404 |     770.638µs | 190.122.106.218 | POST     \"/api/chat\"\n",
            "t=2025-02-05T20:19:59+0000 lvl=info msg=\"join connections\" obj=join id=5b2f0eaa2709 l=127.0.0.1:11434 r=190.122.106.218:33444\n",
            "[GIN] 2025/02/05 - 20:19:59 | 200 |     233.513µs | 190.122.106.218 | GET      \"/api/tags\"\n",
            "time=2025-02-05T20:19:59.958Z level=INFO source=download.go:175 msg=\"downloading 4420ccb0f1d9 in 16 205 MB part(s)\"\n",
            "time=2025-02-05T20:20:17.221Z level=INFO source=download.go:175 msg=\"downloading 66b9ea09bd5b in 1 68 B part(s)\"\n",
            "time=2025-02-05T20:20:18.681Z level=INFO source=download.go:175 msg=\"downloading eb4402837c78 in 1 1.5 KB part(s)\"\n",
            "time=2025-02-05T20:20:19.968Z level=INFO source=download.go:175 msg=\"downloading b5c0e5cf74cf in 1 7.4 KB part(s)\"\n",
            "time=2025-02-05T20:20:21.210Z level=INFO source=download.go:175 msg=\"downloading 8e9087e8cb7f in 1 485 B part(s)\"\n",
            "[GIN] 2025/02/05 - 20:20:33 | 200 | 34.342295076s | 190.122.106.218 | POST     \"/api/pull\"\n",
            "t=2025-02-05T20:20:34+0000 lvl=info msg=\"join connections\" obj=join id=8c522d2bfc0c l=127.0.0.1:11434 r=190.122.106.218:36813\n",
            "time=2025-02-05T20:20:34.197Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-4420ccb0f1d9e12811d04ae2a28ec881469305f813c62d86c10e595ef8e0111d gpu=GPU-83d1565a-856c-4425-03cf-49ec8110cc5a parallel=4 available=15720382464 required=\"4.2 GiB\"\n",
            "time=2025-02-05T20:20:34.289Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-02-05T20:20:34.290Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=37 layers.offload=37 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"4.2 GiB\" memory.required.partial=\"4.2 GiB\" memory.required.kv=\"288.0 MiB\" memory.required.allocations=\"[4.2 GiB]\" memory.weights.total=\"3.0 GiB\" memory.weights.repeating=\"2.7 GiB\" memory.weights.nonrepeating=\"315.3 MiB\" memory.graph.full=\"300.8 MiB\" memory.graph.partial=\"544.2 MiB\"\n",
            "time=2025-02-05T20:20:34.290Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-4420ccb0f1d9e12811d04ae2a28ec881469305f813c62d86c10e595ef8e0111d --ctx-size 8192 --batch-size 512 --n-gpu-layers 37 --threads 1 --parallel 4 --port 37683\"\n",
            "time=2025-02-05T20:20:34.293Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-02-05T20:20:34.293Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-02-05T20:20:34.293Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-02-05T20:20:34.863Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-02-05T20:20:34.882Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-02-05T20:20:34.882Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:37683\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 434 tensors from /root/.ollama/models/blobs/sha256-4420ccb0f1d9e12811d04ae2a28ec881469305f813c62d86c10e595ef8e0111d (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                            general.license str              = other\n",
            "llama_model_loader: - kv   7:                       general.license.name str              = qwen-research\n",
            "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-3...\n",
            "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 3B\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-3B\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
            "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  15:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv  16:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  17:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  18:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  19:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  20:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  21:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  22:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  23:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = qwen2\n",
            "time=2025-02-05T20:20:35.048Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type q8_0:  253 tensors\n",
            "llm_load_vocab: special tokens cache size = 22\n",
            "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 151936\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_layer          = 36\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 2\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 3.09 B\n",
            "llm_load_print_meta: model size       = 3.05 GiB (8.50 BPW)\n",
            "llm_load_print_meta: general.name     = Qwen2.5 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
            "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
            "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
            "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 36 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 37/37 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   315.30 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  3127.61 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 1000000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.35 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   300.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    20.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1266\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-02-05T20:20:37.056Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 434 tensors from /root/.ollama/models/blobs/sha256-4420ccb0f1d9e12811d04ae2a28ec881469305f813c62d86c10e595ef8e0111d (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                            general.license str              = other\n",
            "llama_model_loader: - kv   7:                       general.license.name str              = qwen-research\n",
            "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-3...\n",
            "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 3B\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-3B\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
            "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  15:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv  16:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  17:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  18:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  19:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  20:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  21:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  22:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  23:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  31:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type q8_0:  253 tensors\n",
            "llm_load_vocab: special tokens cache size = 22\n",
            "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 151936\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 3.09 B\n",
            "llm_load_print_meta: model size       = 3.05 GiB (8.50 BPW)\n",
            "llm_load_print_meta: general.name     = Qwen2.5 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
            "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
            "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
            "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/02/05 - 20:20:39 | 200 |  5.093242031s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:41 | 200 |  2.381383607s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:42 | 200 |  441.089561ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:42 | 200 |  414.999028ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:44 | 200 |  1.127839213s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:44 | 200 |   478.23162ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:46 | 200 |  1.392157622s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:47 | 200 |  1.474346385s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:48 | 200 |  561.101912ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:49 | 200 |  1.270084634s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:51 | 200 |  1.126410586s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:20:52 | 200 |  1.168338139s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "t=2025-02-05T20:21:05+0000 lvl=info msg=\"join connections\" obj=join id=b7fb9a27958c l=127.0.0.1:11434 r=190.122.106.218:40067\n",
            "[GIN] 2025/02/05 - 20:21:06 | 200 |  1.290770961s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:07 | 200 |  483.688792ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:09 | 200 |  1.372448006s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:10 | 200 |  1.640548641s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:12 | 200 |  1.621274268s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:13 | 200 |  320.072893ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:14 | 200 |  1.015332034s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:17 | 200 |  2.742059178s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:17 | 200 |  345.015671ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:19 | 200 |  1.821543705s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:20 | 200 |  1.178975658s | 190.122.106.218 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:21:21 | 200 |  564.919463ms | 190.122.106.218 | POST     \"/api/chat\"\n",
            "t=2025-02-05T20:22:36+0000 lvl=info msg=\"join connections\" obj=join id=33871932adce l=127.0.0.1:11434 r=45.169.193.142:51032\n",
            "[GIN] 2025/02/05 - 20:22:37 | 200 |  1.027247148s |  45.169.193.142 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:22:38 | 200 |  990.892454ms |  45.169.193.142 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:22:40 | 200 |  1.053672921s |  45.169.193.142 | POST     \"/api/chat\"\n",
            "[GIN] 2025/02/05 - 20:22:41 | 200 |  1.290245375s |  45.169.193.142 | POST     \"/api/chat\"\n"
          ]
        }
      ],
      "source": [
        "# Run multiple tasks concurrently:\n",
        "#  1. Start the Ollama server.\n",
        "#  2. Start ngrok to forward HTTP traffic from the local ollama api running on localhost:11434.\n",
        "#     Instructions come from Ollama doc: https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-use-ollama-with-ngrok\n",
        "await asyncio.gather(\n",
        "    run(['ollama', 'serve']),\n",
        "\n",
        "    # If you don't want to map to a static URL in Ngrok, uncomment line 9 and comment line 10 before running this cell\n",
        "    # run(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434']),\n",
        "    run(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434', '--domain', NGROK_DOMAIN]),\n",
        ")\n",
        "\n",
        "# on the local computer sending the requests, go to windows cmd and then type: [set OLLAMA_HOST=https://open-perch-apparent.ngrok-free.app], then do ollama run llama3 (or some other model)\n",
        "# in linux, type [export OLLAMA_HOST=https://open-perch-apparent.ngrok-free.app] (unset OLLAMA_HOST to delete the env variable)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nqwfg26rN-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}